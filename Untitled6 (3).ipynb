{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip uninstall -y mediapipe\n",
        "!pip install mediapipe\n",
        "!pip install mediapipe kaggle\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade torch torchvision\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import json\n",
        "import zipfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm  # For progress bars\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "\n",
        "import mediapipe as mp\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize MediaPipe Hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=1,\n",
        "    min_detection_confidence=0.3\n",
        ")"
      ],
      "metadata": {
        "id": "aWzY_9MkMF0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Kaggle API\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download and unzip the dataset\n",
        "print(\"Downloading dataset...\")\n",
        "!kaggle datasets download -d prathumarikeri/indian-sign-language-isl\n",
        "print(\"Unzipping dataset...\")\n",
        "!unzip -q indian-sign-language-isl.zip\n",
        "\n",
        "# Define dataset paths\n",
        "DATASET_PATH = 'Indian'\n",
        "PROCESSED_PATH = 'Processed_Dataset'\n",
        "\n",
        "# Clean up old processed data if it exists\n",
        "if os.path.exists(PROCESSED_PATH):\n",
        "    shutil.rmtree(PROCESSED_PATH)\n",
        "\n",
        "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
        "print(\"Setup complete.\")"
      ],
      "metadata": {
        "id": "eKRAA4zTMKp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_hand_bbox(image, padding=30):\n",
        "    \"\"\"\n",
        "    Finds the hand in an image and returns a padded bounding box.\n",
        "    \"\"\"\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = hands.process(image_rgb)\n",
        "\n",
        "    if not results.multi_hand_landmarks:\n",
        "        return None  # No hand found\n",
        "\n",
        "    # Get landmarks\n",
        "    hand_landmarks = results.multi_hand_landmarks[0]\n",
        "\n",
        "    # Get image dimensions\n",
        "    h, w, _ = image.shape\n",
        "\n",
        "    # Find min/max coordinates\n",
        "    x_coords = [landmark.x for landmark in hand_landmarks.landmark]\n",
        "    y_coords = [landmark.y for landmark in hand_landmarks.landmark]\n",
        "\n",
        "    x_min, x_max = min(x_coords), max(x_coords)\n",
        "    y_min, y_max = min(y_coords), max(y_coords)\n",
        "\n",
        "    # Convert normalized coords to pixel coords\n",
        "    x_min_px = int(x_min * w)\n",
        "    x_max_px = int(x_max * w)\n",
        "    y_min_px = int(y_min * h)\n",
        "    y_max_px = int(y_max * h)\n",
        "\n",
        "    # Apply padding\n",
        "    x_min_px = max(0, x_min_px - padding)\n",
        "    y_min_px = max(0, y_min_px - padding)\n",
        "    x_max_px = min(w, x_max_px + padding)\n",
        "    y_max_px = min(h, y_max_px + padding)\n",
        "\n",
        "    # Ensure width and height are positive\n",
        "    if x_min_px >= x_max_px or y_min_px >= y_max_px:\n",
        "        return None\n",
        "\n",
        "    return (x_min_px, y_min_px, x_max_px, y_max_px)\n",
        "\n",
        "def preprocess_dataset(source_path, dest_path, train_split=0.8):\n",
        "    \"\"\"\n",
        "    Processes the entire dataset, crops hands, and splits into train/val.\n",
        "    \"\"\"\n",
        "    # Create train and val directories\n",
        "    train_dir = os.path.join(dest_path, 'train')\n",
        "    val_dir = os.path.join(dest_path, 'val')\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "    # Get all class names (directories)\n",
        "    class_names = [d for d in os.listdir(source_path) if os.path.isdir(os.path.join(source_path, d))]\n",
        "    # Sort them (e.g., 0-9, then A-Z)\n",
        "    class_names.sort()\n",
        "\n",
        "    print(f\"Found classes: {class_names}\")\n",
        "\n",
        "    for class_name in class_names:\n",
        "        class_path = os.path.join(source_path, class_name)\n",
        "        images = glob.glob(os.path.join(class_path, '*.jpg'))\n",
        "\n",
        "        # Create class-specific train/val dirs\n",
        "        train_class_dir = os.path.join(train_dir, class_name)\n",
        "        val_class_dir = os.path.join(val_dir, class_name)\n",
        "        os.makedirs(train_class_dir, exist_ok=True)\n",
        "        os.makedirs(val_class_dir, exist_ok=True)\n",
        "\n",
        "        # Split images\n",
        "        split_idx = int(len(images) * train_split)\n",
        "        train_images = images[:split_idx]\n",
        "        val_images = images[split_idx:]\n",
        "\n",
        "        print(f\"Processing Class {class_name}: {len(train_images)} train, {len(val_images)} val\")\n",
        "\n",
        "        # Process training images\n",
        "        for img_path in tqdm(train_images, desc=f\"Train {class_name}\"):\n",
        "            image = cv2.imread(img_path)\n",
        "            bbox = get_hand_bbox(image)\n",
        "            if bbox:\n",
        "                x1, y1, x2, y2 = bbox\n",
        "                cropped_hand = image[y1:y2, x1:x2]\n",
        "                if cropped_hand.size > 0:\n",
        "                    save_path = os.path.join(train_class_dir, os.path.basename(img_path))\n",
        "                    cv2.imwrite(save_path, cropped_hand)\n",
        "\n",
        "        # Process validation images\n",
        "        for img_path in tqdm(val_images, desc=f\"Val {class_name}\"):\n",
        "            image = cv2.imread(img_path)\n",
        "            bbox = get_hand_bbox(image)\n",
        "            if bbox:\n",
        "                x1, y1, x2, y2 = bbox\n",
        "                cropped_hand = image[y1:y2, x1:x2]\n",
        "                if cropped_hand.size > 0:\n",
        "                    save_path = os.path.join(val_class_dir, os.path.basename(img_path))\n",
        "                    cv2.imwrite(save_path, cropped_hand)\n",
        "\n",
        "# Run the preprocessing\n",
        "print(\"Starting dataset preprocessing...\")\n",
        "preprocess_dataset(DATASET_PATH, PROCESSED_PATH)\n",
        "print(\"Preprocessing finished.\")"
      ],
      "metadata": {
        "id": "_ktHfzIXaltX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 4: STRONGER AUGMENTATION ---\n",
        "INPUT_SIZE = 224\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        # This forces the model to learn hands at different zooms/positions\n",
        "        transforms.RandomResizedCrop(INPUT_SIZE, scale=(0.6, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(30), # Increased rotation\n",
        "        # Simulates different webcam lighting conditions\n",
        "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Create ImageFolder datasets\n",
        "train_dataset = ImageFolder(os.path.join(PROCESSED_PATH, 'train'), data_transforms['train'])\n",
        "val_dataset = ImageFolder(os.path.join(PROCESSED_PATH, 'val'), data_transforms['val'])\n",
        "\n",
        "# Create DataLoaders\n",
        "BATCH_SIZE = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "class_names = train_dataset.classes\n",
        "num_classes = len(class_names)\n",
        "print(f\"Classes: {class_names}\")"
      ],
      "metadata": {
        "id": "7n35c3x7aqaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 5: FINE-TUNING RESNET ---\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# 1. First, freeze everything\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. UNFREEZE the last block (Layer 4) and the fully connected layer\n",
        "# This allows the model to learn hand-specific features, not just generic shapes\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Use a lower learning rate for fine-tuning\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.layer4.parameters(), 'lr': 1e-4}, # Lower LR for feature layers\n",
        "    {'params': model.fc.parameters(), 'lr': 1e-3}      # Higher LR for classifier\n",
        "])\n",
        "\n",
        "# Training loop\n",
        "NUM_EPOCHS = 20\n",
        "best_val_acc = 0.0\n",
        "model_path = \"best_gesture_model_resnet.pth\"\n",
        "\n",
        "print(\"Starting fine-tuning...\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_dataset)\n",
        "    epoch_train_acc = running_corrects.double() / len(train_dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    running_val_corrects = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_val_loss += loss.item() * inputs.size(0)\n",
        "            running_val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(val_dataset)\n",
        "    epoch_val_acc = running_val_corrects.double() / len(val_dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Train Acc: {epoch_train_acc:.4f} | Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "    if epoch_val_acc >= best_val_acc:\n",
        "        best_val_acc = epoch_val_acc\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "id": "39jgpJ-_as-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Colab Webcam Snippet (Fixed Imports) ---\n",
        "from IPython.display import display, Javascript, Image as IPImage, clear_output\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# Set device again just to be safe\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Load Model ---\n",
        "# Ensure this path matches where your model is saved\n",
        "model_path = \"best_gesture_model_resnet.pth\"\n",
        "\n",
        "# Re-create the model architecture\n",
        "# (We need to know the number of classes. If you ran Step 4, 'class_names' exists.\n",
        "# If not, we assume 36 for 0-9 + A-Z, or 10 for just 0-9. Let's try to grab it dynamically).\n",
        "try:\n",
        "    num_classes = len(class_names)\n",
        "except NameError:\n",
        "    print(\"Warning: class_names not found. Assuming 10 classes (0-9). Change if needed.\")\n",
        "    num_classes = 10 # Default fall back\n",
        "\n",
        "model = models.resnet18()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    print(\"Model loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Model file not found. Please make sure you ran the training step!\")\n",
        "    # Create a dummy model just so the code doesn't crash immediately\n",
        "    pass\n",
        "\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define transforms for live input\n",
        "# (Must match validation transforms)\n",
        "INPUT_SIZE = 224\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "live_transform = transforms.Compose([\n",
        "    transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "def js_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    Converts a base64 image string from Javascript to an OpenCV image.\n",
        "    \"\"\"\n",
        "    image_bytes = b64decode(js_reply.split(',')[1])\n",
        "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "    img = cv2.imdecode(jpg_as_np, flags=cv2.IMREAD_COLOR)\n",
        "    return img\n",
        "\n",
        "def video_stream():\n",
        "    js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function startVideo() {\n",
        "      shutdown = false;\n",
        "      div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      div.appendChild(video);\n",
        "\n",
        "      labelElement = document.createElement('div');\n",
        "      labelElement.innerText = 'Initializing...';\n",
        "      labelElement.style.color = 'green';\n",
        "      labelElement.style.fontSize = '20px';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      div.appendChild(labelElement);\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640;\n",
        "      captureCanvas.height = 480;\n",
        "\n",
        "      stream = navigator.mediaDevices.getUserMedia({video: true})\n",
        "        .then(function(s) {\n",
        "          video.srcObject = s;\n",
        "          video.play();\n",
        "          stream = s;\n",
        "\n",
        "          video.onloadedmetadata = () => {\n",
        "             if (pendingResolve) {\n",
        "                pendingResolve(true);\n",
        "                pendingResolve = null;\n",
        "             }\n",
        "          };\n",
        "        });\n",
        "    }\n",
        "\n",
        "    function stopVideo() {\n",
        "      shutdown = true;\n",
        "      if (stream) {\n",
        "        stream.getTracks().forEach(track => track.stop());\n",
        "        video.srcObject = null;\n",
        "        stream = null;\n",
        "      }\n",
        "      if (div) {\n",
        "        div.remove();\n",
        "        div = null;\n",
        "      }\n",
        "    }\n",
        "\n",
        "    function captureFrame() {\n",
        "      if (shutdown) {\n",
        "        return Promise.resolve(null);\n",
        "      }\n",
        "      if (!captureCanvas) {\n",
        "        return Promise.resolve(null);\n",
        "      }\n",
        "\n",
        "      captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "      var data = captureCanvas.toDataURL('image/jpeg', 0.8);\n",
        "      return Promise.resolve(data);\n",
        "    }\n",
        "\n",
        "    function updateLabel(text) {\n",
        "        if (labelElement) {\n",
        "            labelElement.innerText = text;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    window.google_colab_notebook = {\n",
        "      start: startVideo,\n",
        "      stop: stopVideo,\n",
        "      capture: captureFrame,\n",
        "      update_label: updateLabel\n",
        "    };\n",
        "    ''')\n",
        "    display(js)\n",
        "\n",
        "# --- Main Loop ---\n",
        "\n",
        "# 1. Inject Javascript\n",
        "video_stream()\n",
        "\n",
        "# 2. Start Webcam\n",
        "eval_js('google_colab_notebook.start()')\n",
        "\n",
        "# 3. Init MediaPipe\n",
        "import mediapipe as mp\n",
        "mp_hands = mp.solutions.hands\n",
        "live_hands = mp_hands.Hands(\n",
        "    static_image_mode=False,\n",
        "    max_num_hands=1,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n",
        "\n",
        "print(\"Webcam started...\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        # Capture frame\n",
        "        js_reply = eval_js('google_colab_notebook.capture()')\n",
        "        if not js_reply:\n",
        "            break\n",
        "\n",
        "        frame = js_to_image(js_reply)\n",
        "        frame = cv2.flip(frame, 1)\n",
        "\n",
        "        # MediaPipe\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = live_hands.process(frame_rgb)\n",
        "\n",
        "        prediction_text = \"No Hand Detected\"\n",
        "\n",
        "        if results.multi_hand_landmarks:\n",
        "            hand_landmarks = results.multi_hand_landmarks[0]\n",
        "\n",
        "            # BBox\n",
        "            h, w, _ = frame.shape\n",
        "            x_coords = [lm.x for lm in hand_landmarks.landmark]\n",
        "            y_coords = [lm.y for lm in hand_landmarks.landmark]\n",
        "\n",
        "            # Padding\n",
        "            pad = 20\n",
        "            x_min = int(min(x_coords) * w) - pad\n",
        "            y_min = int(min(y_coords) * h) - pad\n",
        "            x_max = int(max(x_coords) * w) + pad\n",
        "            y_max = int(max(y_coords) * h) + pad\n",
        "\n",
        "            # Clip to image boundaries\n",
        "            x_min = max(0, x_min)\n",
        "            y_min = max(0, y_min)\n",
        "            x_max = min(w, x_max)\n",
        "            y_max = min(h, y_max)\n",
        "\n",
        "            if x_max - x_min > 10 and y_max - y_min > 10:\n",
        "                # Crop\n",
        "                cropped = frame[y_min:y_max, x_min:x_max]\n",
        "                cropped_rgb = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)\n",
        "                pil_img = Image.fromarray(cropped_rgb)\n",
        "\n",
        "                # Predict\n",
        "                input_tensor = live_transform(pil_img).unsqueeze(0).to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(input_tensor)\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                    # Get class name if available\n",
        "                    idx = predicted.item()\n",
        "                    if 'class_names' in globals() and idx < len(class_names):\n",
        "                        label = class_names[idx]\n",
        "                    else:\n",
        "                        label = str(idx)\n",
        "\n",
        "                    prediction_text = f\"Prediction: {label}\"\n",
        "\n",
        "        # Update label in browser\n",
        "        eval_js(f'google_colab_notebook.update_label(\"{prediction_text}\")')\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Stopped.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "finally:\n",
        "    eval_js('google_colab_notebook.stop()')\n",
        "    live_hands.close()"
      ],
      "metadata": {
        "id": "aWCQAKXrCEz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQougw35CRHA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}